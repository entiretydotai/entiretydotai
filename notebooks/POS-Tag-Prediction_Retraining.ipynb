{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from src import FlairDataset\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm_notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../data/external/real_fake_disaster/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-05 15:12:06,797 Reading data from /home/uv/Documents/meetup(in_progress)/entiretydotai/data/interim\n",
      "2020-04-05 15:12:06,798 Train: /home/uv/Documents/meetup(in_progress)/entiretydotai/data/interim/train.csv\n",
      "2020-04-05 15:12:06,799 Dev: /home/uv/Documents/meetup(in_progress)/entiretydotai/data/interim/valid.csv\n",
      "2020-04-05 15:12:06,800 Test: /home/uv/Documents/meetup(in_progress)/entiretydotai/data/interim/test.csv\n"
     ]
    }
   ],
   "source": [
    "dataset = FlairDataset.csv_classification(\n",
    "    data_folder=path, filename='data', column_mapping=['text', 'label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tag Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-05 15:12:07,899 loading file /home/uv/.flair/models/en-ner-fast-conll03-v0.4.pt\n",
      "2020-04-05 15:12:09,026 loading file /home/uv/.flair/models/en-pos-ontonotes-fast-v0.4.pt\n"
     ]
    }
   ],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "# load the NER tagger\n",
    "\n",
    "tagger_ner = SequenceTagger.load('ner-fast')\n",
    "tagger_pos = SequenceTagger.load('pos-fast')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6859</th>\n",
       "      <td>0</td>\n",
       "      <td>@AshGhebranious civil rights continued in the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6751</th>\n",
       "      <td>0</td>\n",
       "      <td>Dakota Skye gets horny with some porn then get...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7281</th>\n",
       "      <td>1</td>\n",
       "      <td>Richard returns after whirlwind few days http:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>909</th>\n",
       "      <td>0</td>\n",
       "      <td>Bloody Mary in the sink. Beet juice http://t.c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5939</th>\n",
       "      <td>0</td>\n",
       "      <td>@Real_Liam_Payne I SCREAMED AT THE TOP OF MY L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4419</th>\n",
       "      <td>0</td>\n",
       "      <td>Mom is hijacking my account to earn MCR STATUS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5418</th>\n",
       "      <td>0</td>\n",
       "      <td>Just realized that maybe it not normal to sit ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3440</th>\n",
       "      <td>0</td>\n",
       "      <td>Im Dead!!! My two Loves in 1 photo! My Heart e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1905</th>\n",
       "      <td>0</td>\n",
       "      <td>Nick Williams just hit another bomb. Just crus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3416</th>\n",
       "      <td>0</td>\n",
       "      <td>Philadelphia EaglesÛª Jordan Matthews Is Goin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                               text\n",
       "6859      0  @AshGhebranious civil rights continued in the ...\n",
       "6751      0  Dakota Skye gets horny with some porn then get...\n",
       "7281      1  Richard returns after whirlwind few days http:...\n",
       "909       0  Bloody Mary in the sink. Beet juice http://t.c...\n",
       "5939      0  @Real_Liam_Payne I SCREAMED AT THE TOP OF MY L...\n",
       "4419      0  Mom is hijacking my account to earn MCR STATUS...\n",
       "5418      0  Just realized that maybe it not normal to sit ...\n",
       "3440      0  Im Dead!!! My two Loves in 1 photo! My Heart e...\n",
       "1905      0  Nick Williams just hit another bomb. Just crus...\n",
       "3416      0  Philadelphia EaglesÛª Jordan Matthews Is Goin..."
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = dataset.train_data[:10][['label', 'text']]\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_tag(row):\n",
    "    sentence = Sentence(row['text'],use_tokenizer=True)\n",
    "    temp = tagger_ner.predict(sentence)\n",
    "    row['ner_tag'] = sentence.to_tagged_string()\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tag(row):\n",
    "    sentence = Sentence(row['text'],use_tokenizer=True)\n",
    "    temp = tagger_pos.predict(sentence)\n",
    "    row['pos_tag'] = sentence.to_tagged_string()\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.apply(ner_tag, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>ner_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6859</th>\n",
       "      <td>0</td>\n",
       "      <td>@AshGhebranious civil rights continued in the ...</td>\n",
       "      <td>@ AshGhebranious &lt;S-MISC&gt; civil rights continu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6751</th>\n",
       "      <td>0</td>\n",
       "      <td>Dakota Skye gets horny with some porn then get...</td>\n",
       "      <td>Dakota &lt;B-PER&gt; Skye &lt;E-PER&gt; gets horny with so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7281</th>\n",
       "      <td>1</td>\n",
       "      <td>Richard returns after whirlwind few days http:...</td>\n",
       "      <td>Richard &lt;S-PER&gt; returns after whirlwind few da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>909</th>\n",
       "      <td>0</td>\n",
       "      <td>Bloody Mary in the sink. Beet juice http://t.c...</td>\n",
       "      <td>Bloody Mary &lt;S-PER&gt; in the sink . Beet juice h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5939</th>\n",
       "      <td>0</td>\n",
       "      <td>@Real_Liam_Payne I SCREAMED AT THE TOP OF MY L...</td>\n",
       "      <td>@ Real _ Liam &lt;S-PER&gt; _ Payne &lt;S-PER&gt; I SCREAM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4419</th>\n",
       "      <td>0</td>\n",
       "      <td>Mom is hijacking my account to earn MCR STATUS...</td>\n",
       "      <td>Mom &lt;S-PER&gt; is hijacking my account to earn MC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5418</th>\n",
       "      <td>0</td>\n",
       "      <td>Just realized that maybe it not normal to sit ...</td>\n",
       "      <td>Just realized that maybe it not normal to sit ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3440</th>\n",
       "      <td>0</td>\n",
       "      <td>Im Dead!!! My two Loves in 1 photo! My Heart e...</td>\n",
       "      <td>Im Dead !! ! My two Loves in 1 photo ! My Hear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1905</th>\n",
       "      <td>0</td>\n",
       "      <td>Nick Williams just hit another bomb. Just crus...</td>\n",
       "      <td>Nick &lt;B-PER&gt; Williams &lt;E-PER&gt; just hit another...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3416</th>\n",
       "      <td>0</td>\n",
       "      <td>Philadelphia EaglesÛª Jordan Matthews Is Goin...</td>\n",
       "      <td>Philadelphia &lt;B-ORG&gt; Eagles &lt;E-ORG&gt; ‰ Û ª Jord...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                               text  \\\n",
       "6859      0  @AshGhebranious civil rights continued in the ...   \n",
       "6751      0  Dakota Skye gets horny with some porn then get...   \n",
       "7281      1  Richard returns after whirlwind few days http:...   \n",
       "909       0  Bloody Mary in the sink. Beet juice http://t.c...   \n",
       "5939      0  @Real_Liam_Payne I SCREAMED AT THE TOP OF MY L...   \n",
       "4419      0  Mom is hijacking my account to earn MCR STATUS...   \n",
       "5418      0  Just realized that maybe it not normal to sit ...   \n",
       "3440      0  Im Dead!!! My two Loves in 1 photo! My Heart e...   \n",
       "1905      0  Nick Williams just hit another bomb. Just crus...   \n",
       "3416      0  Philadelphia EaglesÛª Jordan Matthews Is Goin...   \n",
       "\n",
       "                                                ner_tag  \n",
       "6859  @ AshGhebranious <S-MISC> civil rights continu...  \n",
       "6751  Dakota <B-PER> Skye <E-PER> gets horny with so...  \n",
       "7281  Richard <S-PER> returns after whirlwind few da...  \n",
       "909   Bloody Mary <S-PER> in the sink . Beet juice h...  \n",
       "5939  @ Real _ Liam <S-PER> _ Payne <S-PER> I SCREAM...  \n",
       "4419  Mom <S-PER> is hijacking my account to earn MC...  \n",
       "5418  Just realized that maybe it not normal to sit ...  \n",
       "3440  Im Dead !! ! My two Loves in 1 photo ! My Hear...  \n",
       "1905  Nick <B-PER> Williams <E-PER> just hit another...  \n",
       "3416  Philadelphia <B-ORG> Eagles <E-ORG> ‰ Û ª Jord...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.apply(pos_tag, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used with flair get_spans\n",
    "# tags are not in format similar to BILOU \n",
    "# we get the score for each tags\n",
    "def ner_tags_updated(rows,):\n",
    "    sentence = Sentence(rows[\"text\"],use_tokenizer=True)\n",
    "    temp = tagger_ner.predict(sentence)\n",
    "    text = sentence.to_tokenized_string().split(\" \")\n",
    "    entity_tagged = sentence.get_spans('ner')\n",
    "    tagged_text = [ent.text for ent in entity_tagged]\n",
    "    tagged_label = [ent.tag for ent in entity_tagged]\n",
    "    tagged_score = [ent.score for ent in entity_tagged]\n",
    "    corpus = []\n",
    "    cleaned_ner_tag = []\n",
    "    score = []\n",
    "    for i in text:\n",
    "        if i in tagged_text:\n",
    "            corpus.append(i)\n",
    "            index = tagged_text.index(i)\n",
    "            cleaned_ner_tag.append(tagged_label[index])\n",
    "            score.append(round(tagged_score[index],2))\n",
    "            \n",
    "        else:\n",
    "            corpus.append(i)\n",
    "            cleaned_ner_tag.append(\"NA\")\n",
    "            score.append(np.NaN)\n",
    "    rows[\"updated_ner_corpus\"] = corpus\n",
    "    rows[\"updated_cleaned_ner\"] = cleaned_ner_tag\n",
    "    rows[\"updated_ner_score\"] = score\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>ner_tag</th>\n",
       "      <th>pos_tag</th>\n",
       "      <th>updated_ner_corpus</th>\n",
       "      <th>updated_cleaned_ner</th>\n",
       "      <th>updated_ner_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6859</th>\n",
       "      <td>0</td>\n",
       "      <td>@AshGhebranious civil rights continued in the ...</td>\n",
       "      <td>@ AshGhebranious &lt;S-MISC&gt; civil rights continu...</td>\n",
       "      <td>@ &lt;SYM&gt; AshGhebranious &lt;ADJ&gt; civil &lt;ADJ&gt; right...</td>\n",
       "      <td>[@, AshGhebranious, civil, rights, continued, ...</td>\n",
       "      <td>[NA, MISC, NA, NA, NA, NA, NA, NA, NA, NA, NA,...</td>\n",
       "      <td>[nan, 0.7, nan, nan, nan, nan, nan, nan, nan, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6751</th>\n",
       "      <td>0</td>\n",
       "      <td>Dakota Skye gets horny with some porn then get...</td>\n",
       "      <td>Dakota &lt;B-PER&gt; Skye &lt;E-PER&gt; gets horny with so...</td>\n",
       "      <td>Dakota &lt;PROPN&gt; Skye &lt;PROPN&gt; gets &lt;VERB&gt; horny ...</td>\n",
       "      <td>[Dakota, Skye, gets, horny, with, some, porn, ...</td>\n",
       "      <td>[NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N...</td>\n",
       "      <td>[nan, nan, nan, nan, nan, nan, nan, nan, nan, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7281</th>\n",
       "      <td>1</td>\n",
       "      <td>Richard returns after whirlwind few days http:...</td>\n",
       "      <td>Richard &lt;S-PER&gt; returns after whirlwind few da...</td>\n",
       "      <td>Richard &lt;PROPN&gt; returns &lt;VERB&gt; after &lt;ADP&gt; whi...</td>\n",
       "      <td>[Richard, returns, after, whirlwind, few, days...</td>\n",
       "      <td>[PER, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ...</td>\n",
       "      <td>[0.98, nan, nan, nan, nan, nan, nan, nan, nan,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>909</th>\n",
       "      <td>0</td>\n",
       "      <td>Bloody Mary in the sink. Beet juice http://t.c...</td>\n",
       "      <td>Bloody Mary &lt;S-PER&gt; in the sink . Beet juice h...</td>\n",
       "      <td>Bloody &lt;ADJ&gt; Mary &lt;PROPN&gt; in &lt;ADP&gt; the &lt;DET&gt; s...</td>\n",
       "      <td>[Bloody, Mary, in, the, sink, ., Beet, juice, ...</td>\n",
       "      <td>[NA, PER, NA, NA, NA, NA, NA, NA, NA, NA, NA, ...</td>\n",
       "      <td>[nan, 0.64, nan, nan, nan, nan, nan, nan, nan,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5939</th>\n",
       "      <td>0</td>\n",
       "      <td>@Real_Liam_Payne I SCREAMED AT THE TOP OF MY L...</td>\n",
       "      <td>@ Real _ Liam &lt;S-PER&gt; _ Payne &lt;S-PER&gt; I SCREAM...</td>\n",
       "      <td>@ &lt;SYM&gt; Real &lt;PROPN&gt; _ &lt;SYM&gt; Liam &lt;PROPN&gt; _ &lt;S...</td>\n",
       "      <td>[@, Real, _, Liam, _, Payne, I, SCREAMED, AT, ...</td>\n",
       "      <td>[NA, NA, NA, PER, NA, PER, NA, NA, NA, NA, NA,...</td>\n",
       "      <td>[nan, nan, nan, 0.64, nan, 0.99, nan, nan, nan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4419</th>\n",
       "      <td>0</td>\n",
       "      <td>Mom is hijacking my account to earn MCR STATUS...</td>\n",
       "      <td>Mom &lt;S-PER&gt; is hijacking my account to earn MC...</td>\n",
       "      <td>Mom &lt;PROPN&gt; is &lt;VERB&gt; hijacking &lt;VERB&gt; my &lt;PRO...</td>\n",
       "      <td>[Mom, is, hijacking, my, account, to, earn, MC...</td>\n",
       "      <td>[PER, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ...</td>\n",
       "      <td>[0.92, nan, nan, nan, nan, nan, nan, nan, nan,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5418</th>\n",
       "      <td>0</td>\n",
       "      <td>Just realized that maybe it not normal to sit ...</td>\n",
       "      <td>Just realized that maybe it not normal to sit ...</td>\n",
       "      <td>Just &lt;ADV&gt; realized &lt;VERB&gt; that &lt;ADP&gt; maybe &lt;A...</td>\n",
       "      <td>[Just, realized, that, maybe, it, not, normal,...</td>\n",
       "      <td>[NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N...</td>\n",
       "      <td>[nan, nan, nan, nan, nan, nan, nan, nan, nan, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3440</th>\n",
       "      <td>0</td>\n",
       "      <td>Im Dead!!! My two Loves in 1 photo! My Heart e...</td>\n",
       "      <td>Im Dead !! ! My two Loves in 1 photo ! My Hear...</td>\n",
       "      <td>Im &lt;VERB&gt; Dead &lt;ADJ&gt; !! &lt;PUNCT&gt; ! &lt;PUNCT&gt; My &lt;...</td>\n",
       "      <td>[Im, Dead, !!, !, My, two, Loves, in, 1, photo...</td>\n",
       "      <td>[NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N...</td>\n",
       "      <td>[nan, nan, nan, nan, nan, nan, nan, nan, nan, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1905</th>\n",
       "      <td>0</td>\n",
       "      <td>Nick Williams just hit another bomb. Just crus...</td>\n",
       "      <td>Nick &lt;B-PER&gt; Williams &lt;E-PER&gt; just hit another...</td>\n",
       "      <td>Nick &lt;PROPN&gt; Williams &lt;PROPN&gt; just &lt;ADV&gt; hit &lt;...</td>\n",
       "      <td>[Nick, Williams, just, hit, another, bomb, ., ...</td>\n",
       "      <td>[NA, NA, NA, NA, NA, NA, NA, NA, NA, NA]</td>\n",
       "      <td>[nan, nan, nan, nan, nan, nan, nan, nan, nan, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3416</th>\n",
       "      <td>0</td>\n",
       "      <td>Philadelphia EaglesÛª Jordan Matthews Is Goin...</td>\n",
       "      <td>Philadelphia &lt;B-ORG&gt; Eagles &lt;E-ORG&gt; ‰ Û ª Jord...</td>\n",
       "      <td>Philadelphia &lt;PROPN&gt; Eagles &lt;PROPN&gt; ‰ &lt;SYM&gt; Û ...</td>\n",
       "      <td>[Philadelphia, Eagles, ‰, Û, ª, Jordan, Matthe...</td>\n",
       "      <td>[NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N...</td>\n",
       "      <td>[nan, nan, nan, nan, nan, nan, nan, nan, nan, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                               text  \\\n",
       "6859      0  @AshGhebranious civil rights continued in the ...   \n",
       "6751      0  Dakota Skye gets horny with some porn then get...   \n",
       "7281      1  Richard returns after whirlwind few days http:...   \n",
       "909       0  Bloody Mary in the sink. Beet juice http://t.c...   \n",
       "5939      0  @Real_Liam_Payne I SCREAMED AT THE TOP OF MY L...   \n",
       "4419      0  Mom is hijacking my account to earn MCR STATUS...   \n",
       "5418      0  Just realized that maybe it not normal to sit ...   \n",
       "3440      0  Im Dead!!! My two Loves in 1 photo! My Heart e...   \n",
       "1905      0  Nick Williams just hit another bomb. Just crus...   \n",
       "3416      0  Philadelphia EaglesÛª Jordan Matthews Is Goin...   \n",
       "\n",
       "                                                ner_tag  \\\n",
       "6859  @ AshGhebranious <S-MISC> civil rights continu...   \n",
       "6751  Dakota <B-PER> Skye <E-PER> gets horny with so...   \n",
       "7281  Richard <S-PER> returns after whirlwind few da...   \n",
       "909   Bloody Mary <S-PER> in the sink . Beet juice h...   \n",
       "5939  @ Real _ Liam <S-PER> _ Payne <S-PER> I SCREAM...   \n",
       "4419  Mom <S-PER> is hijacking my account to earn MC...   \n",
       "5418  Just realized that maybe it not normal to sit ...   \n",
       "3440  Im Dead !! ! My two Loves in 1 photo ! My Hear...   \n",
       "1905  Nick <B-PER> Williams <E-PER> just hit another...   \n",
       "3416  Philadelphia <B-ORG> Eagles <E-ORG> ‰ Û ª Jord...   \n",
       "\n",
       "                                                pos_tag  \\\n",
       "6859  @ <SYM> AshGhebranious <ADJ> civil <ADJ> right...   \n",
       "6751  Dakota <PROPN> Skye <PROPN> gets <VERB> horny ...   \n",
       "7281  Richard <PROPN> returns <VERB> after <ADP> whi...   \n",
       "909   Bloody <ADJ> Mary <PROPN> in <ADP> the <DET> s...   \n",
       "5939  @ <SYM> Real <PROPN> _ <SYM> Liam <PROPN> _ <S...   \n",
       "4419  Mom <PROPN> is <VERB> hijacking <VERB> my <PRO...   \n",
       "5418  Just <ADV> realized <VERB> that <ADP> maybe <A...   \n",
       "3440  Im <VERB> Dead <ADJ> !! <PUNCT> ! <PUNCT> My <...   \n",
       "1905  Nick <PROPN> Williams <PROPN> just <ADV> hit <...   \n",
       "3416  Philadelphia <PROPN> Eagles <PROPN> ‰ <SYM> Û ...   \n",
       "\n",
       "                                     updated_ner_corpus  \\\n",
       "6859  [@, AshGhebranious, civil, rights, continued, ...   \n",
       "6751  [Dakota, Skye, gets, horny, with, some, porn, ...   \n",
       "7281  [Richard, returns, after, whirlwind, few, days...   \n",
       "909   [Bloody, Mary, in, the, sink, ., Beet, juice, ...   \n",
       "5939  [@, Real, _, Liam, _, Payne, I, SCREAMED, AT, ...   \n",
       "4419  [Mom, is, hijacking, my, account, to, earn, MC...   \n",
       "5418  [Just, realized, that, maybe, it, not, normal,...   \n",
       "3440  [Im, Dead, !!, !, My, two, Loves, in, 1, photo...   \n",
       "1905  [Nick, Williams, just, hit, another, bomb, ., ...   \n",
       "3416  [Philadelphia, Eagles, ‰, Û, ª, Jordan, Matthe...   \n",
       "\n",
       "                                    updated_cleaned_ner  \\\n",
       "6859  [NA, MISC, NA, NA, NA, NA, NA, NA, NA, NA, NA,...   \n",
       "6751  [NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N...   \n",
       "7281  [PER, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ...   \n",
       "909   [NA, PER, NA, NA, NA, NA, NA, NA, NA, NA, NA, ...   \n",
       "5939  [NA, NA, NA, PER, NA, PER, NA, NA, NA, NA, NA,...   \n",
       "4419  [PER, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ...   \n",
       "5418  [NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N...   \n",
       "3440  [NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N...   \n",
       "1905           [NA, NA, NA, NA, NA, NA, NA, NA, NA, NA]   \n",
       "3416  [NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N...   \n",
       "\n",
       "                                      updated_ner_score  \n",
       "6859  [nan, 0.7, nan, nan, nan, nan, nan, nan, nan, ...  \n",
       "6751  [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...  \n",
       "7281  [0.98, nan, nan, nan, nan, nan, nan, nan, nan,...  \n",
       "909   [nan, 0.64, nan, nan, nan, nan, nan, nan, nan,...  \n",
       "5939  [nan, nan, nan, 0.64, nan, 0.99, nan, nan, nan...  \n",
       "4419  [0.92, nan, nan, nan, nan, nan, nan, nan, nan,...  \n",
       "5418  [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...  \n",
       "3440  [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...  \n",
       "1905  [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...  \n",
       "3416  [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = test.apply(ner_tags_updated,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_tags(rows,):\n",
    "    for j in ['<', '>']:\n",
    "        rows = str(rows).replace(j, \"\")\n",
    "        rows = re.sub(' +', ' ', str(rows))\n",
    "        rows = str(rows).strip()\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pos_tags(rows,):\n",
    "    text = rows['pos_tag'].split(\" \")\n",
    "    corpus = [i for i in text if not i.strip().startswith(\"<\")]\n",
    "    tags = [clean_tags(i) for i in text if i.strip().startswith(\"<\")]\n",
    "    if len(corpus)== len(tags):\n",
    "        rows['pos_corpus'] = corpus\n",
    "        rows['cleaned_pos_tags'] = tags\n",
    "    return rows\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>ner_tag</th>\n",
       "      <th>pos_tag</th>\n",
       "      <th>pos_corpus</th>\n",
       "      <th>cleaned_pos_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6859</th>\n",
       "      <td>0</td>\n",
       "      <td>@AshGhebranious civil rights continued in the ...</td>\n",
       "      <td>@ AshGhebranious &lt;S-MISC&gt; civil rights continu...</td>\n",
       "      <td>@ &lt;SYM&gt; AshGhebranious &lt;ADJ&gt; civil &lt;ADJ&gt; right...</td>\n",
       "      <td>[@, AshGhebranious, civil, rights, continued, ...</td>\n",
       "      <td>[SYM, ADJ, ADJ, NOUN, VERB, ADP, DET, NOUN, PU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6751</th>\n",
       "      <td>0</td>\n",
       "      <td>Dakota Skye gets horny with some porn then get...</td>\n",
       "      <td>Dakota &lt;B-PER&gt; Skye &lt;E-PER&gt; gets horny with so...</td>\n",
       "      <td>Dakota &lt;PROPN&gt; Skye &lt;PROPN&gt; gets &lt;VERB&gt; horny ...</td>\n",
       "      <td>[Dakota, Skye, gets, horny, with, some, porn, ...</td>\n",
       "      <td>[PROPN, PROPN, VERB, ADJ, ADP, DET, NOUN, ADV,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7281</th>\n",
       "      <td>1</td>\n",
       "      <td>Richard returns after whirlwind few days http:...</td>\n",
       "      <td>Richard &lt;S-PER&gt; returns after whirlwind few da...</td>\n",
       "      <td>Richard &lt;PROPN&gt; returns &lt;VERB&gt; after &lt;ADP&gt; whi...</td>\n",
       "      <td>[Richard, returns, after, whirlwind, few, days...</td>\n",
       "      <td>[PROPN, VERB, ADP, ADJ, ADJ, NOUN, X, X, X, SY...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>909</th>\n",
       "      <td>0</td>\n",
       "      <td>Bloody Mary in the sink. Beet juice http://t.c...</td>\n",
       "      <td>Bloody Mary &lt;S-PER&gt; in the sink . Beet juice h...</td>\n",
       "      <td>Bloody &lt;ADJ&gt; Mary &lt;PROPN&gt; in &lt;ADP&gt; the &lt;DET&gt; s...</td>\n",
       "      <td>[Bloody, Mary, in, the, sink, ., Beet, juice, ...</td>\n",
       "      <td>[ADJ, PROPN, ADP, DET, NOUN, PUNCT, NOUN, NOUN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5939</th>\n",
       "      <td>0</td>\n",
       "      <td>@Real_Liam_Payne I SCREAMED AT THE TOP OF MY L...</td>\n",
       "      <td>@ Real _ Liam &lt;S-PER&gt; _ Payne &lt;S-PER&gt; I SCREAM...</td>\n",
       "      <td>@ &lt;SYM&gt; Real &lt;PROPN&gt; _ &lt;SYM&gt; Liam &lt;PROPN&gt; _ &lt;S...</td>\n",
       "      <td>[@, Real, _, Liam, _, Payne, I, SCREAMED, AT, ...</td>\n",
       "      <td>[SYM, PROPN, SYM, PROPN, SYM, PROPN, PRON, VER...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4419</th>\n",
       "      <td>0</td>\n",
       "      <td>Mom is hijacking my account to earn MCR STATUS...</td>\n",
       "      <td>Mom &lt;S-PER&gt; is hijacking my account to earn MC...</td>\n",
       "      <td>Mom &lt;PROPN&gt; is &lt;VERB&gt; hijacking &lt;VERB&gt; my &lt;PRO...</td>\n",
       "      <td>[Mom, is, hijacking, my, account, to, earn, MC...</td>\n",
       "      <td>[PROPN, VERB, VERB, PRON, NOUN, PART, VERB, PR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5418</th>\n",
       "      <td>0</td>\n",
       "      <td>Just realized that maybe it not normal to sit ...</td>\n",
       "      <td>Just realized that maybe it not normal to sit ...</td>\n",
       "      <td>Just &lt;ADV&gt; realized &lt;VERB&gt; that &lt;ADP&gt; maybe &lt;A...</td>\n",
       "      <td>[Just, realized, that, maybe, it, not, normal,...</td>\n",
       "      <td>[ADV, VERB, ADP, ADV, PRON, ADV, ADJ, PART, VE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3440</th>\n",
       "      <td>0</td>\n",
       "      <td>Im Dead!!! My two Loves in 1 photo! My Heart e...</td>\n",
       "      <td>Im Dead !! ! My two Loves in 1 photo ! My Hear...</td>\n",
       "      <td>Im &lt;VERB&gt; Dead &lt;ADJ&gt; !! &lt;PUNCT&gt; ! &lt;PUNCT&gt; My &lt;...</td>\n",
       "      <td>[Im, Dead, !!, !, My, two, Loves, in, 1, photo...</td>\n",
       "      <td>[VERB, ADJ, PUNCT, PUNCT, PRON, NUM, NOUN, ADP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1905</th>\n",
       "      <td>0</td>\n",
       "      <td>Nick Williams just hit another bomb. Just crus...</td>\n",
       "      <td>Nick &lt;B-PER&gt; Williams &lt;E-PER&gt; just hit another...</td>\n",
       "      <td>Nick &lt;PROPN&gt; Williams &lt;PROPN&gt; just &lt;ADV&gt; hit &lt;...</td>\n",
       "      <td>[Nick, Williams, just, hit, another, bomb, ., ...</td>\n",
       "      <td>[PROPN, PROPN, ADV, VERB, DET, NOUN, PUNCT, AD...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3416</th>\n",
       "      <td>0</td>\n",
       "      <td>Philadelphia EaglesÛª Jordan Matthews Is Goin...</td>\n",
       "      <td>Philadelphia &lt;B-ORG&gt; Eagles &lt;E-ORG&gt; ‰ Û ª Jord...</td>\n",
       "      <td>Philadelphia &lt;PROPN&gt; Eagles &lt;PROPN&gt; ‰ &lt;SYM&gt; Û ...</td>\n",
       "      <td>[Philadelphia, Eagles, ‰, Û, ª, Jordan, Matthe...</td>\n",
       "      <td>[PROPN, PROPN, SYM, SYM, NUM, PROPN, PROPN, VE...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                               text  \\\n",
       "6859      0  @AshGhebranious civil rights continued in the ...   \n",
       "6751      0  Dakota Skye gets horny with some porn then get...   \n",
       "7281      1  Richard returns after whirlwind few days http:...   \n",
       "909       0  Bloody Mary in the sink. Beet juice http://t.c...   \n",
       "5939      0  @Real_Liam_Payne I SCREAMED AT THE TOP OF MY L...   \n",
       "4419      0  Mom is hijacking my account to earn MCR STATUS...   \n",
       "5418      0  Just realized that maybe it not normal to sit ...   \n",
       "3440      0  Im Dead!!! My two Loves in 1 photo! My Heart e...   \n",
       "1905      0  Nick Williams just hit another bomb. Just crus...   \n",
       "3416      0  Philadelphia EaglesÛª Jordan Matthews Is Goin...   \n",
       "\n",
       "                                                ner_tag  \\\n",
       "6859  @ AshGhebranious <S-MISC> civil rights continu...   \n",
       "6751  Dakota <B-PER> Skye <E-PER> gets horny with so...   \n",
       "7281  Richard <S-PER> returns after whirlwind few da...   \n",
       "909   Bloody Mary <S-PER> in the sink . Beet juice h...   \n",
       "5939  @ Real _ Liam <S-PER> _ Payne <S-PER> I SCREAM...   \n",
       "4419  Mom <S-PER> is hijacking my account to earn MC...   \n",
       "5418  Just realized that maybe it not normal to sit ...   \n",
       "3440  Im Dead !! ! My two Loves in 1 photo ! My Hear...   \n",
       "1905  Nick <B-PER> Williams <E-PER> just hit another...   \n",
       "3416  Philadelphia <B-ORG> Eagles <E-ORG> ‰ Û ª Jord...   \n",
       "\n",
       "                                                pos_tag  \\\n",
       "6859  @ <SYM> AshGhebranious <ADJ> civil <ADJ> right...   \n",
       "6751  Dakota <PROPN> Skye <PROPN> gets <VERB> horny ...   \n",
       "7281  Richard <PROPN> returns <VERB> after <ADP> whi...   \n",
       "909   Bloody <ADJ> Mary <PROPN> in <ADP> the <DET> s...   \n",
       "5939  @ <SYM> Real <PROPN> _ <SYM> Liam <PROPN> _ <S...   \n",
       "4419  Mom <PROPN> is <VERB> hijacking <VERB> my <PRO...   \n",
       "5418  Just <ADV> realized <VERB> that <ADP> maybe <A...   \n",
       "3440  Im <VERB> Dead <ADJ> !! <PUNCT> ! <PUNCT> My <...   \n",
       "1905  Nick <PROPN> Williams <PROPN> just <ADV> hit <...   \n",
       "3416  Philadelphia <PROPN> Eagles <PROPN> ‰ <SYM> Û ...   \n",
       "\n",
       "                                             pos_corpus  \\\n",
       "6859  [@, AshGhebranious, civil, rights, continued, ...   \n",
       "6751  [Dakota, Skye, gets, horny, with, some, porn, ...   \n",
       "7281  [Richard, returns, after, whirlwind, few, days...   \n",
       "909   [Bloody, Mary, in, the, sink, ., Beet, juice, ...   \n",
       "5939  [@, Real, _, Liam, _, Payne, I, SCREAMED, AT, ...   \n",
       "4419  [Mom, is, hijacking, my, account, to, earn, MC...   \n",
       "5418  [Just, realized, that, maybe, it, not, normal,...   \n",
       "3440  [Im, Dead, !!, !, My, two, Loves, in, 1, photo...   \n",
       "1905  [Nick, Williams, just, hit, another, bomb, ., ...   \n",
       "3416  [Philadelphia, Eagles, ‰, Û, ª, Jordan, Matthe...   \n",
       "\n",
       "                                       cleaned_pos_tags  \n",
       "6859  [SYM, ADJ, ADJ, NOUN, VERB, ADP, DET, NOUN, PU...  \n",
       "6751  [PROPN, PROPN, VERB, ADJ, ADP, DET, NOUN, ADV,...  \n",
       "7281  [PROPN, VERB, ADP, ADJ, ADJ, NOUN, X, X, X, SY...  \n",
       "909   [ADJ, PROPN, ADP, DET, NOUN, PUNCT, NOUN, NOUN...  \n",
       "5939  [SYM, PROPN, SYM, PROPN, SYM, PROPN, PRON, VER...  \n",
       "4419  [PROPN, VERB, VERB, PRON, NOUN, PART, VERB, PR...  \n",
       "5418  [ADV, VERB, ADP, ADV, PRON, ADV, ADJ, PART, VE...  \n",
       "3440  [VERB, ADJ, PUNCT, PUNCT, PRON, NUM, NOUN, ADP...  \n",
       "1905  [PROPN, PROPN, ADV, VERB, DET, NOUN, PUNCT, AD...  \n",
       "3416  [PROPN, PROPN, SYM, SYM, NUM, PROPN, PROPN, VE...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = test.apply(extract_pos_tags,axis=1)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if used Sentence.to_tagged_string()\n",
    "def extract_ner_tags(rows,):\n",
    "    text = rows['ner_tag'].split(\" \")\n",
    "    #print(text)\n",
    "    tot_words = len(text)\n",
    "    #print(tot_words)\n",
    "    words = []\n",
    "    tags = []\n",
    "    for i,wd in enumerate(text):\n",
    "        if wd.startswith(\"<\"):\n",
    "            continue\n",
    "#         print(words)\n",
    "#         print(tags)\n",
    "        if i+1 < tot_words:\n",
    "            #print(i)\n",
    "            if text[i+1].startswith(\"<\"):\n",
    "#                 print(wd)\n",
    "                words.append(wd)\n",
    "                tags.append(clean_tags(text[i+1]))\n",
    "            else:\n",
    "                #print(wd)\n",
    "                words.append(wd)\n",
    "                tags.append(\"NA\")\n",
    "        \n",
    "        else:\n",
    "            if not text[i].startswith(\"<\"):\n",
    "                words.append(wd)\n",
    "                tags.append(\"NA\")\n",
    "                \n",
    "    if len(words) == len(tags):\n",
    "        rows['ner_corpus'] = words\n",
    "        rows['cleaned_ner_tags'] = tags\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.apply(extract_ner_tags,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>ner_tag</th>\n",
       "      <th>pos_tag</th>\n",
       "      <th>pos_corpus</th>\n",
       "      <th>cleaned_pos_tags</th>\n",
       "      <th>ner_corpus</th>\n",
       "      <th>cleaned_ner_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6859</th>\n",
       "      <td>0</td>\n",
       "      <td>@AshGhebranious civil rights continued in the ...</td>\n",
       "      <td>@ AshGhebranious &lt;S-MISC&gt; civil rights continu...</td>\n",
       "      <td>@ &lt;SYM&gt; AshGhebranious &lt;ADJ&gt; civil &lt;ADJ&gt; right...</td>\n",
       "      <td>[@, AshGhebranious, civil, rights, continued, ...</td>\n",
       "      <td>[SYM, ADJ, ADJ, NOUN, VERB, ADP, DET, NOUN, PU...</td>\n",
       "      <td>[@, AshGhebranious, civil, rights, continued, ...</td>\n",
       "      <td>[NA, S-MISC, NA, NA, NA, NA, NA, NA, NA, NA, N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6751</th>\n",
       "      <td>0</td>\n",
       "      <td>Dakota Skye gets horny with some porn then get...</td>\n",
       "      <td>Dakota &lt;B-PER&gt; Skye &lt;E-PER&gt; gets horny with so...</td>\n",
       "      <td>Dakota &lt;PROPN&gt; Skye &lt;PROPN&gt; gets &lt;VERB&gt; horny ...</td>\n",
       "      <td>[Dakota, Skye, gets, horny, with, some, porn, ...</td>\n",
       "      <td>[PROPN, PROPN, VERB, ADJ, ADP, DET, NOUN, ADV,...</td>\n",
       "      <td>[Dakota, Skye, gets, horny, with, some, porn, ...</td>\n",
       "      <td>[B-PER, E-PER, NA, NA, NA, NA, NA, NA, NA, NA,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7281</th>\n",
       "      <td>1</td>\n",
       "      <td>Richard returns after whirlwind few days http:...</td>\n",
       "      <td>Richard &lt;S-PER&gt; returns after whirlwind few da...</td>\n",
       "      <td>Richard &lt;PROPN&gt; returns &lt;VERB&gt; after &lt;ADP&gt; whi...</td>\n",
       "      <td>[Richard, returns, after, whirlwind, few, days...</td>\n",
       "      <td>[PROPN, VERB, ADP, ADJ, ADJ, NOUN, X, X, X, SY...</td>\n",
       "      <td>[Richard, returns, after, whirlwind, few, days...</td>\n",
       "      <td>[S-PER, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>909</th>\n",
       "      <td>0</td>\n",
       "      <td>Bloody Mary in the sink. Beet juice http://t.c...</td>\n",
       "      <td>Bloody Mary &lt;S-PER&gt; in the sink . Beet juice h...</td>\n",
       "      <td>Bloody &lt;ADJ&gt; Mary &lt;PROPN&gt; in &lt;ADP&gt; the &lt;DET&gt; s...</td>\n",
       "      <td>[Bloody, Mary, in, the, sink, ., Beet, juice, ...</td>\n",
       "      <td>[ADJ, PROPN, ADP, DET, NOUN, PUNCT, NOUN, NOUN...</td>\n",
       "      <td>[Bloody, Mary, in, the, sink, ., Beet, juice, ...</td>\n",
       "      <td>[NA, S-PER, NA, NA, NA, NA, NA, NA, NA, NA, NA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5939</th>\n",
       "      <td>0</td>\n",
       "      <td>@Real_Liam_Payne I SCREAMED AT THE TOP OF MY L...</td>\n",
       "      <td>@ Real _ Liam &lt;S-PER&gt; _ Payne &lt;S-PER&gt; I SCREAM...</td>\n",
       "      <td>@ &lt;SYM&gt; Real &lt;PROPN&gt; _ &lt;SYM&gt; Liam &lt;PROPN&gt; _ &lt;S...</td>\n",
       "      <td>[@, Real, _, Liam, _, Payne, I, SCREAMED, AT, ...</td>\n",
       "      <td>[SYM, PROPN, SYM, PROPN, SYM, PROPN, PRON, VER...</td>\n",
       "      <td>[@, Real, _, Liam, _, Payne, I, SCREAMED, AT, ...</td>\n",
       "      <td>[NA, NA, NA, S-PER, NA, S-PER, NA, NA, NA, NA,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4419</th>\n",
       "      <td>0</td>\n",
       "      <td>Mom is hijacking my account to earn MCR STATUS...</td>\n",
       "      <td>Mom &lt;S-PER&gt; is hijacking my account to earn MC...</td>\n",
       "      <td>Mom &lt;PROPN&gt; is &lt;VERB&gt; hijacking &lt;VERB&gt; my &lt;PRO...</td>\n",
       "      <td>[Mom, is, hijacking, my, account, to, earn, MC...</td>\n",
       "      <td>[PROPN, VERB, VERB, PRON, NOUN, PART, VERB, PR...</td>\n",
       "      <td>[Mom, is, hijacking, my, account, to, earn, MC...</td>\n",
       "      <td>[S-PER, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5418</th>\n",
       "      <td>0</td>\n",
       "      <td>Just realized that maybe it not normal to sit ...</td>\n",
       "      <td>Just realized that maybe it not normal to sit ...</td>\n",
       "      <td>Just &lt;ADV&gt; realized &lt;VERB&gt; that &lt;ADP&gt; maybe &lt;A...</td>\n",
       "      <td>[Just, realized, that, maybe, it, not, normal,...</td>\n",
       "      <td>[ADV, VERB, ADP, ADV, PRON, ADV, ADJ, PART, VE...</td>\n",
       "      <td>[Just, realized, that, maybe, it, not, normal,...</td>\n",
       "      <td>[NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3440</th>\n",
       "      <td>0</td>\n",
       "      <td>Im Dead!!! My two Loves in 1 photo! My Heart e...</td>\n",
       "      <td>Im Dead !! ! My two Loves in 1 photo ! My Hear...</td>\n",
       "      <td>Im &lt;VERB&gt; Dead &lt;ADJ&gt; !! &lt;PUNCT&gt; ! &lt;PUNCT&gt; My &lt;...</td>\n",
       "      <td>[Im, Dead, !!, !, My, two, Loves, in, 1, photo...</td>\n",
       "      <td>[VERB, ADJ, PUNCT, PUNCT, PRON, NUM, NOUN, ADP...</td>\n",
       "      <td>[Im, Dead, !!, !, My, two, Loves, in, 1, photo...</td>\n",
       "      <td>[NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1905</th>\n",
       "      <td>0</td>\n",
       "      <td>Nick Williams just hit another bomb. Just crus...</td>\n",
       "      <td>Nick &lt;B-PER&gt; Williams &lt;E-PER&gt; just hit another...</td>\n",
       "      <td>Nick &lt;PROPN&gt; Williams &lt;PROPN&gt; just &lt;ADV&gt; hit &lt;...</td>\n",
       "      <td>[Nick, Williams, just, hit, another, bomb, ., ...</td>\n",
       "      <td>[PROPN, PROPN, ADV, VERB, DET, NOUN, PUNCT, AD...</td>\n",
       "      <td>[Nick, Williams, just, hit, another, bomb, ., ...</td>\n",
       "      <td>[B-PER, E-PER, NA, NA, NA, NA, NA, NA, NA, NA]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3416</th>\n",
       "      <td>0</td>\n",
       "      <td>Philadelphia EaglesÛª Jordan Matthews Is Goin...</td>\n",
       "      <td>Philadelphia &lt;B-ORG&gt; Eagles &lt;E-ORG&gt; ‰ Û ª Jord...</td>\n",
       "      <td>Philadelphia &lt;PROPN&gt; Eagles &lt;PROPN&gt; ‰ &lt;SYM&gt; Û ...</td>\n",
       "      <td>[Philadelphia, Eagles, ‰, Û, ª, Jordan, Matthe...</td>\n",
       "      <td>[PROPN, PROPN, SYM, SYM, NUM, PROPN, PROPN, VE...</td>\n",
       "      <td>[Philadelphia, Eagles, ‰, Û, ª, Jordan, Matthe...</td>\n",
       "      <td>[B-ORG, E-ORG, NA, NA, NA, B-PER, E-PER, NA, N...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                               text  \\\n",
       "6859      0  @AshGhebranious civil rights continued in the ...   \n",
       "6751      0  Dakota Skye gets horny with some porn then get...   \n",
       "7281      1  Richard returns after whirlwind few days http:...   \n",
       "909       0  Bloody Mary in the sink. Beet juice http://t.c...   \n",
       "5939      0  @Real_Liam_Payne I SCREAMED AT THE TOP OF MY L...   \n",
       "4419      0  Mom is hijacking my account to earn MCR STATUS...   \n",
       "5418      0  Just realized that maybe it not normal to sit ...   \n",
       "3440      0  Im Dead!!! My two Loves in 1 photo! My Heart e...   \n",
       "1905      0  Nick Williams just hit another bomb. Just crus...   \n",
       "3416      0  Philadelphia EaglesÛª Jordan Matthews Is Goin...   \n",
       "\n",
       "                                                ner_tag  \\\n",
       "6859  @ AshGhebranious <S-MISC> civil rights continu...   \n",
       "6751  Dakota <B-PER> Skye <E-PER> gets horny with so...   \n",
       "7281  Richard <S-PER> returns after whirlwind few da...   \n",
       "909   Bloody Mary <S-PER> in the sink . Beet juice h...   \n",
       "5939  @ Real _ Liam <S-PER> _ Payne <S-PER> I SCREAM...   \n",
       "4419  Mom <S-PER> is hijacking my account to earn MC...   \n",
       "5418  Just realized that maybe it not normal to sit ...   \n",
       "3440  Im Dead !! ! My two Loves in 1 photo ! My Hear...   \n",
       "1905  Nick <B-PER> Williams <E-PER> just hit another...   \n",
       "3416  Philadelphia <B-ORG> Eagles <E-ORG> ‰ Û ª Jord...   \n",
       "\n",
       "                                                pos_tag  \\\n",
       "6859  @ <SYM> AshGhebranious <ADJ> civil <ADJ> right...   \n",
       "6751  Dakota <PROPN> Skye <PROPN> gets <VERB> horny ...   \n",
       "7281  Richard <PROPN> returns <VERB> after <ADP> whi...   \n",
       "909   Bloody <ADJ> Mary <PROPN> in <ADP> the <DET> s...   \n",
       "5939  @ <SYM> Real <PROPN> _ <SYM> Liam <PROPN> _ <S...   \n",
       "4419  Mom <PROPN> is <VERB> hijacking <VERB> my <PRO...   \n",
       "5418  Just <ADV> realized <VERB> that <ADP> maybe <A...   \n",
       "3440  Im <VERB> Dead <ADJ> !! <PUNCT> ! <PUNCT> My <...   \n",
       "1905  Nick <PROPN> Williams <PROPN> just <ADV> hit <...   \n",
       "3416  Philadelphia <PROPN> Eagles <PROPN> ‰ <SYM> Û ...   \n",
       "\n",
       "                                             pos_corpus  \\\n",
       "6859  [@, AshGhebranious, civil, rights, continued, ...   \n",
       "6751  [Dakota, Skye, gets, horny, with, some, porn, ...   \n",
       "7281  [Richard, returns, after, whirlwind, few, days...   \n",
       "909   [Bloody, Mary, in, the, sink, ., Beet, juice, ...   \n",
       "5939  [@, Real, _, Liam, _, Payne, I, SCREAMED, AT, ...   \n",
       "4419  [Mom, is, hijacking, my, account, to, earn, MC...   \n",
       "5418  [Just, realized, that, maybe, it, not, normal,...   \n",
       "3440  [Im, Dead, !!, !, My, two, Loves, in, 1, photo...   \n",
       "1905  [Nick, Williams, just, hit, another, bomb, ., ...   \n",
       "3416  [Philadelphia, Eagles, ‰, Û, ª, Jordan, Matthe...   \n",
       "\n",
       "                                       cleaned_pos_tags  \\\n",
       "6859  [SYM, ADJ, ADJ, NOUN, VERB, ADP, DET, NOUN, PU...   \n",
       "6751  [PROPN, PROPN, VERB, ADJ, ADP, DET, NOUN, ADV,...   \n",
       "7281  [PROPN, VERB, ADP, ADJ, ADJ, NOUN, X, X, X, SY...   \n",
       "909   [ADJ, PROPN, ADP, DET, NOUN, PUNCT, NOUN, NOUN...   \n",
       "5939  [SYM, PROPN, SYM, PROPN, SYM, PROPN, PRON, VER...   \n",
       "4419  [PROPN, VERB, VERB, PRON, NOUN, PART, VERB, PR...   \n",
       "5418  [ADV, VERB, ADP, ADV, PRON, ADV, ADJ, PART, VE...   \n",
       "3440  [VERB, ADJ, PUNCT, PUNCT, PRON, NUM, NOUN, ADP...   \n",
       "1905  [PROPN, PROPN, ADV, VERB, DET, NOUN, PUNCT, AD...   \n",
       "3416  [PROPN, PROPN, SYM, SYM, NUM, PROPN, PROPN, VE...   \n",
       "\n",
       "                                             ner_corpus  \\\n",
       "6859  [@, AshGhebranious, civil, rights, continued, ...   \n",
       "6751  [Dakota, Skye, gets, horny, with, some, porn, ...   \n",
       "7281  [Richard, returns, after, whirlwind, few, days...   \n",
       "909   [Bloody, Mary, in, the, sink, ., Beet, juice, ...   \n",
       "5939  [@, Real, _, Liam, _, Payne, I, SCREAMED, AT, ...   \n",
       "4419  [Mom, is, hijacking, my, account, to, earn, MC...   \n",
       "5418  [Just, realized, that, maybe, it, not, normal,...   \n",
       "3440  [Im, Dead, !!, !, My, two, Loves, in, 1, photo...   \n",
       "1905  [Nick, Williams, just, hit, another, bomb, ., ...   \n",
       "3416  [Philadelphia, Eagles, ‰, Û, ª, Jordan, Matthe...   \n",
       "\n",
       "                                       cleaned_ner_tags  \n",
       "6859  [NA, S-MISC, NA, NA, NA, NA, NA, NA, NA, NA, N...  \n",
       "6751  [B-PER, E-PER, NA, NA, NA, NA, NA, NA, NA, NA,...  \n",
       "7281  [S-PER, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA...  \n",
       "909   [NA, S-PER, NA, NA, NA, NA, NA, NA, NA, NA, NA...  \n",
       "5939  [NA, NA, NA, S-PER, NA, S-PER, NA, NA, NA, NA,...  \n",
       "4419  [S-PER, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA...  \n",
       "5418  [NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N...  \n",
       "3440  [NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N...  \n",
       "1905     [B-PER, E-PER, NA, NA, NA, NA, NA, NA, NA, NA]  \n",
       "3416  [B-ORG, E-ORG, NA, NA, NA, B-PER, E-PER, NA, N...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6859    True\n",
       "6751    True\n",
       "7281    True\n",
       "909     True\n",
       "5939    True\n",
       "4419    True\n",
       "5418    True\n",
       "3440    True\n",
       "1905    True\n",
       "3416    True\n",
       "dtype: bool"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.ner_corpus == test.pos_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Conll data fromat for training our own corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading conll dataset\n",
    "\n",
    "The data file contains one word per line, with empty lines representing sentence boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/external/pos_tag_retraining/conll.train', 'r') as f:\n",
    "    txt = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['-DOCSTART- -X- -X- O',\n",
       " '',\n",
       " 'EU NNP I-NP I-ORG',\n",
       " 'rejects VBZ I-VP O',\n",
       " 'German JJ I-NP I-MISC',\n",
       " 'call NN I-NP O',\n",
       " 'to TO I-VP O',\n",
       " 'boycott VB I-VP O',\n",
       " 'British JJ I-NP I-MISC',\n",
       " 'lamb NN I-NP O']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt.split(\"\\n\")[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = txt.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = [x for x in txt if x != '-DOCSTART- -X- -X- O']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'EU NNP I-NP I-ORG',\n",
       " 'rejects VBZ I-VP O',\n",
       " 'German JJ I-NP I-MISC',\n",
       " 'call NN I-NP O',\n",
       " 'to TO I-VP O',\n",
       " 'boycott VB I-VP O',\n",
       " 'British JJ I-NP I-MISC',\n",
       " 'lamb NN I-NP O',\n",
       " '. . O O']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/uv/.local/lib/python3.6/site-packages/ipykernel_launcher.py:6: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d0677eac4dc440c9b4398117e3a55f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=218609.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize empty list for storing words\n",
    "words = []\n",
    "# initialize empty list for storing sentences #\n",
    "corpus = []\n",
    "\n",
    "for i in tqdm_notebook(txt):\n",
    "    if i == '':\n",
    "        ## previous words form a sentence ##\n",
    "        corpus.append(' '.join(words))\n",
    "        ## Refresh Word list ##\n",
    "        words = []\n",
    "    else:\n",
    "       ## word at index 0 ##\n",
    "        words.append(i.split()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'EU rejects German call to boycott British lamb .',\n",
       " 'Peter Blackburn',\n",
       " 'BRUSSELS 1996-08-22',\n",
       " 'The European Commission said on Thursday it disagreed with German advice to consumers to shun British lamb until scientists determine whether mad cow disease can be transmitted to sheep .',\n",
       " \"Germany 's representative to the European Union 's veterinary committee Werner Zwingmann said on Wednesday consumers should buy sheepmeat from countries other than Britain until the scientific advice was clearer .\",\n",
       " '\" We do n\\'t support any such recommendation because we do n\\'t see any grounds for it , \" the Commission \\'s chief spokesman Nikolaus van der Pas told a news briefing .',\n",
       " 'He said further scientific study was required and if it was found that action was needed it should be taken by the European Union .',\n",
       " 'He said a proposal last month by EU Farm Commissioner Franz Fischler to ban sheep brains , spleens and spinal cords from the human and animal food chains was a highly specific and precautionary move to protect human health .',\n",
       " 'Fischler proposed EU-wide measures after reports from Britain and France that under laboratory conditions sheep could contract Bovine Spongiform Encephalopathy ( BSE ) -- mad cow disease .']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [x for x in corpus if x != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/uv/.local/lib/python3.6/site-packages/ipykernel_launcher.py:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "089f392ee9134974b202568c46496a5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=218609.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize empty list for storing word pos\n",
    "w_pos = []\n",
    "#initialize empty list for storing sentence pos #\n",
    "POS = []\n",
    "for i in tqdm_notebook(txt):\n",
    "  ## blank sentence = new line ##\n",
    "    if i == '':\n",
    "        ## previous words form a sentence POS ##\n",
    "        POS.append(' '.join(w_pos))\n",
    "    ## Refresh words list ##\n",
    "        w_pos = []\n",
    "    else:\n",
    "        ## pos tag from index 1 ##\n",
    "        w_pos.append(i.split()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS = [x for x in POS if x != '']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flair Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/uv/.local/lib/python3.6/site-packages/ipykernel_launcher.py:2: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4a68edf965f417a984b1c2908a9c3bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "f_pos = []\n",
    "for i in tqdm_notebook(corpus[:10]):\n",
    "    sentence = Sentence(i)\n",
    "    tagger_pos.predict(sentence)\n",
    "  ## append tagged sentence ##\n",
    "    f_pos.append(sentence.to_tagged_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['EU <PROPN> rejects <VERB> German <ADJ> call <NOUN> to <PART> boycott <VERB> British <ADJ> lamb <NOUN> . <PUNCT>']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_pos[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/uv/.local/lib/python3.6/site-packages/ipykernel_launcher.py:1: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71f8bf40a249474d92633b0b9cd73d83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm_notebook(range(len(f_pos))):\n",
    "    ## for every words ith sentence ##\n",
    "    for j in corpus[i].split():\n",
    "        ## replace that word from ith sentence in f_pos ##\n",
    "        f_pos[i] = str(f_pos[i]).replace(j, \"\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' <PROPN>  <VERB>  <ADJ>  <NOUN>  <PART>  <VERB>  <ADJ>  <NOUN>  <PUNCT>']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_pos[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/uv/.local/lib/python3.6/site-packages/ipykernel_launcher.py:1: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20793bc867a1414bae9feb2ff3cd1d3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "f_pos = [clean_tags(i) for i in tqdm_notebook(f_pos)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PROPN VERB ADJ NOUN PART VERB ADJ NOUN PUNCT']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_pos[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['EU rejects German call to boycott British lamb .'],\n",
       " ['PROPN VERB ADJ NOUN PART VERB ADJ NOUN PUNCT'])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:1],f_pos[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ADJ'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_pos[:1][0].split(\" \")[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PROPN VERB ADJ NOUN PART VERB ADJ NOUN PUNCT'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_pos[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'EU rejects German call to boycott British lamb .'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting our dataset in conll dataset format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['label', 'text', 'ner_tag', 'pos_tag', 'pos_corpus', 'cleaned_pos_tags',\n",
       "       'ner_corpus', 'cleaned_ner_tags'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/uv/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7ba71da9dab4817b43d5583be65a670",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#pos_tags\n",
    "train_corpus = \"\"\n",
    "for row in tqdm_notebook(test.itertuples()):\n",
    "    words = \"\"\n",
    "    corpus = row.pos_corpus\n",
    "    pos_tags = row.cleaned_pos_tags\n",
    "    ner_tags = row.cleaned_ner_tags\n",
    "    for j, word in enumerate(corpus):\n",
    "        txt_tag = str(word) + \" \" + pos_tags[j] + \" \" + ner_tags[j]\n",
    "        words = words + \"\\n\" + txt_tag\n",
    "    train_corpus = train_corpus + \"\\n\" + words\n",
    "train_corpus = train_corpus[2:]    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@AshGhebranious ADJ NA\n",
      "civil ADJ NA\n",
      "rights NOUN NA\n",
      "continued VERB NA\n",
      "in ADP NA\n",
      "the DET NA\n",
      "60s. NOUN NA\n",
      "And CCONJ NA\n",
      "what PRON NA\n",
      "about ADP NA\n",
      "trans-generational ADJ NA\n",
      "trauma? NOUN NA\n",
      "if ADP NA\n",
      "anything NOUN NA\n",
      "we PRON NA\n",
      "should AUX NA\n",
      "listen VERB NA\n",
      "to ADP NA\n",
      "the DET NA\n",
      "Americans. PROPN S-MISC\n",
      "\n",
      "Dakota PROPN B-PER\n",
      "Skye PROPN NA\n",
      "gets VERB E-PER\n",
      "horny ADJ NA\n",
      "with ADP NA\n",
      "some DET NA\n",
      "porn NOUN NA\n",
      "then ADV NA\n",
      "gets VERB NA\n",
      "her PRON NA\n",
      "juicy ADJ NA\n",
      "pussy NOUN NA\n",
      "pounded VERB NA\n",
      "http://t.co/qew4c5M1xd NOUN NA\n",
      "View NOUN NA\n",
      "and CCONJ NA\n",
      "download VERB NA\n",
      "video NOUN NA\n",
      "\n",
      "Richard PROPN S-PER\n",
      "returns VERB NA\n",
      "after ADP NA\n",
      "whirlwind ADJ NA\n",
      "few ADJ NA\n",
      "days NOUN NA\n",
      "http://t.co/L8W30WFW3R NOUN NA\n",
      "#MLB NOUN NA\n",
      "\n",
      "Bloody ADJ NA\n",
      "Mary PROPN S-PER\n",
      "in ADP NA\n",
      "the DET NA\n",
      "sink. NOUN NA\n",
      "Beet NOUN NA\n",
      "juice NOUN NA\n",
      "http://t.co/LUigmHMa1i X NA\n",
      "\n",
      "@Real_Liam_Payne INTJ NA\n",
      "I PRON NA\n",
      "SCREAMED VERB NA\n",
      "AT ADP NA\n",
      "THE DET NA\n",
      "TOP NOUN NA\n",
      "OF ADP NA\n",
      "MY PRON NA\n",
      "LUNGS NOUN NA\n",
      "WHEN ADV NA\n",
      "YOU PRON NA\n",
      "SAID VERB NA\n",
      "YOU PRON NA\n",
      "GUYS NOUN NA\n",
      "WOULD AUX NA\n",
      "COME VERB NA\n",
      "BACK ADV NA\n",
      "TO ADP NA\n",
      "S.A PROPN NA\n",
      "SO ADV NA\n",
      "KEEP VERB NA\n",
      "YOUR PRON NA\n",
      "PROMISE NOUN NA\n",
      "#AddTexasToNext1DTour X NA\n",
      "\n",
      "Mom PROPN S-PER\n",
      "is VERB NA\n",
      "hijacking VERB NA\n",
      "my PRON NA\n",
      "account NOUN NA\n",
      "to PART NA\n",
      "earn VERB NA\n",
      "MCR PROPN NA\n",
      "STATUS!!! PROPN NA\n",
      "Get VERB NA\n",
      "your PRON NA\n",
      "own ADJ NA\n",
      "account NOUN NA\n",
      "snort! X NA\n",
      "\n",
      "http://t.co/jST5hAUK35 X NA\n",
      "#FlavorChargedTea X NA\n",
      "\n",
      "Just ADV NA\n",
      "realized VERB NA\n",
      "that ADP NA\n",
      "maybe ADV NA\n",
      "it PRON NA\n",
      "not ADV NA\n",
      "normal ADJ NA\n",
      "to PART NA\n",
      "sit VERB NA\n",
      "up ADP NA\n",
      "front NOUN NA\n",
      "with ADP NA\n",
      "an DET NA\n",
      "Uber PROPN S-MISC\n",
      "driver? NOUN NA\n",
      "Panicking PROPN NA\n",
      "\n",
      "Im VERB NA\n",
      "Dead!!! PROPN NA\n",
      "My PRON NA\n",
      "two NUM NA\n",
      "Loves NOUN NA\n",
      "in ADP NA\n",
      "1 NUM NA\n",
      "photo! NOUN NA\n",
      "My PRON NA\n",
      "Heart NOUN NA\n",
      "exploded VERB NA\n",
      "into ADP NA\n",
      "a DET NA\n",
      "Million NOUN NA\n",
      "Pieces!!! PUNCT NA\n",
      "?????????????? X NA\n",
      "@BrandonSkeie X NA\n",
      "@samsmithworld X NA\n",
      "http://t.co/yEtagC2d8A X NA\n",
      "\n",
      "Nick PROPN B-PER\n",
      "Williams PROPN NA\n",
      "just ADV E-PER\n",
      "hit VERB NA\n",
      "another DET NA\n",
      "bomb. NOUN NA\n",
      "Just ADV NA\n",
      "crushed VERB NA\n",
      "it PRON NA\n",
      "\n",
      "Philadelphia PROPN S-ORG\n",
      "Eagles‰Ûª PROPN NA\n",
      "Jordan PROPN NA\n",
      "Matthews PROPN B-PER\n",
      "Is VERB NA\n",
      "Going VERB E-PER\n",
      "To PART NA\n",
      "Explode VERB NA\n",
      "In ADP NA\n",
      "2015 NUM NA\n",
      "http://t.co/rRq1ildkiL NOUN NA\n",
      "#news NOUN NA\n",
      "#hotnewscake NOUN NA\n"
     ]
    }
   ],
   "source": [
    "print(train_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retraining POS Tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-04 23:53:17,935 loading file /home/uv/.flair/models/en-pos-ontonotes-fast-v0.4.pt\n"
     ]
    }
   ],
   "source": [
    "tagger = SequenceTagger.load(\"pos-fast\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/interim/pos_train.txt\",'w') as f:\n",
    "    f.write(train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-04 23:55:02,573 Reading data from ../data/interim\n",
      "2020-04-04 23:55:02,575 Train: ../data/interim/pos_train.txt\n",
      "2020-04-04 23:55:02,578 Dev: None\n",
      "2020-04-04 23:55:02,580 Test: None\n"
     ]
    }
   ],
   "source": [
    "from flair.data import Corpus\n",
    "from flair.datasets import ColumnCorpus\n",
    "columns = {0: 'text', 1: 'pos', 2: 'ner'}\n",
    "data_folder = \"../data/interim\"\n",
    "corpus: Corpus = ColumnCorpus(data_folder, columns, train_file='pos_train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_type = 'pos'\n",
    "tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'item2idx': {b'<unk>': 0,\n",
       "  b'O': 1,\n",
       "  b'PROPN': 2,\n",
       "  b'VERB': 3,\n",
       "  b'PART': 4,\n",
       "  b'ADP': 5,\n",
       "  b'NUM': 6,\n",
       "  b'NOUN': 7,\n",
       "  b'INTJ': 8,\n",
       "  b'PRON': 9,\n",
       "  b'DET': 10,\n",
       "  b'ADV': 11,\n",
       "  b'AUX': 12,\n",
       "  b'X': 13,\n",
       "  b'ADJ': 14,\n",
       "  b'CCONJ': 15,\n",
       "  b'PUNCT': 16,\n",
       "  b'<START>': 17,\n",
       "  b'<STOP>': 18},\n",
       " 'idx2item': [b'<unk>',\n",
       "  b'O',\n",
       "  b'PROPN',\n",
       "  b'VERB',\n",
       "  b'PART',\n",
       "  b'ADP',\n",
       "  b'NUM',\n",
       "  b'NOUN',\n",
       "  b'INTJ',\n",
       "  b'PRON',\n",
       "  b'DET',\n",
       "  b'ADV',\n",
       "  b'AUX',\n",
       "  b'X',\n",
       "  b'ADJ',\n",
       "  b'CCONJ',\n",
       "  b'PUNCT',\n",
       "  b'<START>',\n",
       "  b'<STOP>'],\n",
       " 'multi_label': False}"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(tag_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-04 23:59:29,013 ----------------------------------------------------------------------------------------------------\n",
      "2020-04-04 23:59:29,018 Model: \"SequenceTagger(\n",
      "  (embeddings): StackedEmbeddings(\n",
      "    (list_embedding_0): FlairEmbeddings(\n",
      "      (lm): LanguageModel(\n",
      "        (drop): Dropout(p=0.25, inplace=False)\n",
      "        (encoder): Embedding(275, 100)\n",
      "        (rnn): LSTM(100, 1024)\n",
      "        (decoder): Linear(in_features=1024, out_features=275, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (list_embedding_1): FlairEmbeddings(\n",
      "      (lm): LanguageModel(\n",
      "        (drop): Dropout(p=0.25, inplace=False)\n",
      "        (encoder): Embedding(275, 100)\n",
      "        (rnn): LSTM(100, 1024)\n",
      "        (decoder): Linear(in_features=1024, out_features=275, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (word_dropout): WordDropout(p=0.05)\n",
      "  (locked_dropout): LockedDropout(p=0.5)\n",
      "  (embedding2nn): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "  (rnn): LSTM(2048, 256, batch_first=True, bidirectional=True)\n",
      "  (linear): Linear(in_features=512, out_features=20, bias=True)\n",
      "  (beta): 1.0\n",
      "  (weights): None\n",
      "  (weight_tensor) None\n",
      ")\"\n",
      "2020-04-04 23:59:29,019 ----------------------------------------------------------------------------------------------------\n",
      "2020-04-04 23:59:29,020 Corpus: \"Corpus: 9 train + 1 dev + 1 test sentences\"\n",
      "2020-04-04 23:59:29,022 ----------------------------------------------------------------------------------------------------\n",
      "2020-04-04 23:59:29,023 Parameters:\n",
      "2020-04-04 23:59:29,024  - learning_rate: \"0.1\"\n",
      "2020-04-04 23:59:29,025  - mini_batch_size: \"32\"\n",
      "2020-04-04 23:59:29,026  - patience: \"3\"\n",
      "2020-04-04 23:59:29,027  - anneal_factor: \"0.5\"\n",
      "2020-04-04 23:59:29,029  - max_epochs: \"1\"\n",
      "2020-04-04 23:59:29,030  - shuffle: \"True\"\n",
      "2020-04-04 23:59:29,030  - train_with_dev: \"False\"\n",
      "2020-04-04 23:59:29,033  - batch_growth_annealing: \"False\"\n",
      "2020-04-04 23:59:29,034 ----------------------------------------------------------------------------------------------------\n",
      "2020-04-04 23:59:29,034 Model training base path: \"../models/retraining/pos/flair_pos_test\"\n",
      "2020-04-04 23:59:29,036 ----------------------------------------------------------------------------------------------------\n",
      "2020-04-04 23:59:29,037 Device: cpu\n",
      "2020-04-04 23:59:29,038 ----------------------------------------------------------------------------------------------------\n",
      "2020-04-04 23:59:29,038 Embeddings storage mode: cpu\n",
      "2020-04-04 23:59:29,040 ----------------------------------------------------------------------------------------------------\n",
      "2020-04-04 23:59:31,107 epoch 1 - iter 1/1 - loss 2.44234729 - samples/sec: 15.50\n",
      "2020-04-04 23:59:31,123 ----------------------------------------------------------------------------------------------------\n",
      "2020-04-04 23:59:31,124 EPOCH 1 done: loss 2.4423 - lr 0.1000\n",
      "2020-04-04 23:59:31,366 DEV : loss 1.20416259765625 - score 1.0\n",
      "2020-04-04 23:59:31,367 BAD EPOCHS (no improvement): 0\n",
      "2020-04-04 23:59:31,516 ----------------------------------------------------------------------------------------------------\n",
      "2020-04-04 23:59:31,517 Testing using best model ...\n",
      "2020-04-04 23:59:31,517 loading file ../models/retraining/pos/flair_pos_test/best-model.pt\n",
      "2020-04-04 23:59:31,854 1.0\t1.0\t1.0\n",
      "2020-04-04 23:59:31,855 \n",
      "MICRO_AVG: acc 1.0 - f1-score 1.0\n",
      "MACRO_AVG: acc 1.0 - f1-score 1.0\n",
      "ADP        tp: 2 - fp: 0 - fn: 0 - tn: 2 - precision: 1.0000 - recall: 1.0000 - accuracy: 1.0000 - f1-score: 1.0000\n",
      "DET        tp: 1 - fp: 0 - fn: 0 - tn: 1 - precision: 1.0000 - recall: 1.0000 - accuracy: 1.0000 - f1-score: 1.0000\n",
      "NOUN       tp: 4 - fp: 0 - fn: 0 - tn: 4 - precision: 1.0000 - recall: 1.0000 - accuracy: 1.0000 - f1-score: 1.0000\n",
      "NUM        tp: 2 - fp: 0 - fn: 0 - tn: 2 - precision: 1.0000 - recall: 1.0000 - accuracy: 1.0000 - f1-score: 1.0000\n",
      "PRON       tp: 2 - fp: 0 - fn: 0 - tn: 2 - precision: 1.0000 - recall: 1.0000 - accuracy: 1.0000 - f1-score: 1.0000\n",
      "PROPN      tp: 1 - fp: 0 - fn: 0 - tn: 1 - precision: 1.0000 - recall: 1.0000 - accuracy: 1.0000 - f1-score: 1.0000\n",
      "PUNCT      tp: 1 - fp: 0 - fn: 0 - tn: 1 - precision: 1.0000 - recall: 1.0000 - accuracy: 1.0000 - f1-score: 1.0000\n",
      "VERB       tp: 2 - fp: 0 - fn: 0 - tn: 2 - precision: 1.0000 - recall: 1.0000 - accuracy: 1.0000 - f1-score: 1.0000\n",
      "X          tp: 4 - fp: 0 - fn: 0 - tn: 4 - precision: 1.0000 - recall: 1.0000 - accuracy: 1.0000 - f1-score: 1.0000\n",
      "2020-04-04 23:59:31,855 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_score': 1.0,\n",
       " 'dev_score_history': [1.0],\n",
       " 'train_loss_history': [2.442347288131714],\n",
       " 'dev_loss_history': [tensor(1.2042)]}"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from flair.trainers import ModelTrainer\n",
    "trainer: ModelTrainer = ModelTrainer(tagger, corpus)\n",
    "trainer.train('../models/retraining/pos/flair_pos_test',\n",
    "              train_with_dev=False, max_epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retraining NER Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-05 00:53:27,397 loading file /home/uv/.flair/models/en-ner-fast-conll03-v0.4.pt\n"
     ]
    }
   ],
   "source": [
    "tagger = SequenceTagger.load(\"ner-fast\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-05 00:54:15,678 Reading data from ../data/interim\n",
      "2020-04-05 00:54:15,683 Train: ../data/interim/pos_train.txt\n",
      "2020-04-05 00:54:15,686 Dev: None\n",
      "2020-04-05 00:54:15,688 Test: None\n"
     ]
    }
   ],
   "source": [
    "corpus: Corpus = ColumnCorpus(data_folder, columns, train_file='pos_train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_type = 'ner'\n",
    "tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'item2idx': {b'<unk>': 0,\n",
       "  b'O': 1,\n",
       "  b'NA': 2,\n",
       "  b'S-PER': 3,\n",
       "  b'B-PER': 4,\n",
       "  b'E-PER': 5,\n",
       "  b'S-MISC': 6,\n",
       "  b'S-ORG': 7,\n",
       "  b'<START>': 8,\n",
       "  b'<STOP>': 9},\n",
       " 'idx2item': [b'<unk>',\n",
       "  b'O',\n",
       "  b'NA',\n",
       "  b'S-PER',\n",
       "  b'B-PER',\n",
       "  b'E-PER',\n",
       "  b'S-MISC',\n",
       "  b'S-ORG',\n",
       "  b'<START>',\n",
       "  b'<STOP>'],\n",
       " 'multi_label': False}"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(tag_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-05 00:55:11,586 ----------------------------------------------------------------------------------------------------\n",
      "2020-04-05 00:55:11,588 Model: \"SequenceTagger(\n",
      "  (embeddings): StackedEmbeddings(\n",
      "    (list_embedding_0): WordEmbeddings('glove')\n",
      "    (list_embedding_1): FlairEmbeddings(\n",
      "      (lm): LanguageModel(\n",
      "        (drop): Dropout(p=0.25, inplace=False)\n",
      "        (encoder): Embedding(275, 100)\n",
      "        (rnn): LSTM(100, 1024)\n",
      "        (decoder): Linear(in_features=1024, out_features=275, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (list_embedding_2): FlairEmbeddings(\n",
      "      (lm): LanguageModel(\n",
      "        (drop): Dropout(p=0.25, inplace=False)\n",
      "        (encoder): Embedding(275, 100)\n",
      "        (rnn): LSTM(100, 1024)\n",
      "        (decoder): Linear(in_features=1024, out_features=275, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (word_dropout): WordDropout(p=0.05)\n",
      "  (locked_dropout): LockedDropout(p=0.5)\n",
      "  (embedding2nn): Linear(in_features=2148, out_features=2148, bias=True)\n",
      "  (rnn): LSTM(2148, 256, batch_first=True, bidirectional=True)\n",
      "  (linear): Linear(in_features=512, out_features=20, bias=True)\n",
      "  (beta): 1.0\n",
      "  (weights): None\n",
      "  (weight_tensor) None\n",
      ")\"\n",
      "2020-04-05 00:55:11,590 ----------------------------------------------------------------------------------------------------\n",
      "2020-04-05 00:55:11,595 Corpus: \"Corpus: 9 train + 1 dev + 1 test sentences\"\n",
      "2020-04-05 00:55:11,596 ----------------------------------------------------------------------------------------------------\n",
      "2020-04-05 00:55:11,597 Parameters:\n",
      "2020-04-05 00:55:11,598  - learning_rate: \"0.1\"\n",
      "2020-04-05 00:55:11,601  - mini_batch_size: \"32\"\n",
      "2020-04-05 00:55:11,602  - patience: \"3\"\n",
      "2020-04-05 00:55:11,603  - anneal_factor: \"0.5\"\n",
      "2020-04-05 00:55:11,604  - max_epochs: \"1\"\n",
      "2020-04-05 00:55:11,605  - shuffle: \"True\"\n",
      "2020-04-05 00:55:11,608  - train_with_dev: \"False\"\n",
      "2020-04-05 00:55:11,608  - batch_growth_annealing: \"False\"\n",
      "2020-04-05 00:55:11,610 ----------------------------------------------------------------------------------------------------\n",
      "2020-04-05 00:55:11,611 Model training base path: \"../models/retraining/ner/flair_ner_test\"\n",
      "2020-04-05 00:55:11,613 ----------------------------------------------------------------------------------------------------\n",
      "2020-04-05 00:55:11,614 Device: cpu\n",
      "2020-04-05 00:55:11,616 ----------------------------------------------------------------------------------------------------\n",
      "2020-04-05 00:55:11,618 Embeddings storage mode: cpu\n",
      "2020-04-05 00:55:11,619 ----------------------------------------------------------------------------------------------------\n",
      "2020-04-05 00:55:13,196 epoch 1 - iter 1/1 - loss 187.69935608 - samples/sec: 20.31\n",
      "2020-04-05 00:55:13,209 ----------------------------------------------------------------------------------------------------\n",
      "2020-04-05 00:55:13,209 EPOCH 1 done: loss 187.6994 - lr 0.1000\n",
      "2020-04-05 00:55:13,538 DEV : loss 148.37034606933594 - score 0.1333\n",
      "2020-04-05 00:55:13,539 BAD EPOCHS (no improvement): 0\n",
      "2020-04-05 00:55:18,617 ----------------------------------------------------------------------------------------------------\n",
      "2020-04-05 00:55:18,617 Testing using best model ...\n",
      "2020-04-05 00:55:18,618 loading file ../models/retraining/ner/flair_ner_test/best-model.pt\n",
      "2020-04-05 00:55:19,888 0.5\t0.0625\t0.1111\n",
      "2020-04-05 00:55:19,889 \n",
      "MICRO_AVG: acc 0.0588 - f1-score 0.1111\n",
      "MACRO_AVG: acc 0.3333 - f1-score 0.3333333333333333\n",
      "MISC       tp: 1 - fp: 0 - fn: 0 - tn: 1 - precision: 1.0000 - recall: 1.0000 - accuracy: 1.0000 - f1-score: 1.0000\n",
      "NA         tp: 0 - fp: 0 - fn: 15 - tn: 0 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.0000 - f1-score: 0.0000\n",
      "PER        tp: 0 - fp: 1 - fn: 0 - tn: 0 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.0000 - f1-score: 0.0000\n",
      "2020-04-05 00:55:19,890 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_score': 0.1111,\n",
       " 'dev_score_history': [0.1333],\n",
       " 'train_loss_history': [187.69935607910156],\n",
       " 'dev_loss_history': [tensor(148.3703)]}"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from flair.trainers import ModelTrainer\n",
    "trainer: ModelTrainer = ModelTrainer(tagger, corpus)\n",
    "trainer.train('../models/retraining/ner/flair_ner_test',\n",
    "              train_with_dev=False, max_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
